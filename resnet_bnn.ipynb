{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet-bnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXAjSk8ItphR"
      },
      "source": [
        "#Resnet with Bayesian Output Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tke-z-YSc_OJ"
      },
      "source": [
        "## Loading Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb3MSpmOAC9k"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from IPython import display\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5vEAr_pdCH6",
        "outputId": "c3490936-c6be-4640-e782-3a78edf2a8cc"
      },
      "source": [
        "#load trainingset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH = F\"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/testset.pth\"\n",
        "testset = torch.load(PATH,  map_location='cpu')\n",
        "print(len(testset))\n",
        "\n",
        "PATH = F\"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/trainset.pth\"\n",
        "trainset = torch.load(PATH,  map_location='cpu')\n",
        "print(len(trainset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "10000\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAPR3wiFoV9c"
      },
      "source": [
        "## Loading Bayesian Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OcqXOYU-WQq",
        "outputId": "b4a03e7e-405b-4981-dcae-d01e1ed2d9b8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from IPython import display\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "!pip install pillow\n",
        "#from scipy.misc import imread\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "class NN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.out(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WVRHDbJuig6"
      },
      "source": [
        "Initializing Bayesian Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzW94sHRvJZF",
        "outputId": "8bc267cf-1f68-4b70-80ad-1c36e784b8d2"
      },
      "source": [
        "!pip3 install pyro-ppl==1.5.0\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Categorical\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "log_softmax = nn.LogSoftmax(dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl==1.5.0\n",
            "  Downloading pyro_ppl-1.5.0-py3-none-any.whl (604 kB)\n",
            "\u001b[K     |████████████████████████████████| 604 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl==1.5.0) (4.41.1)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl==1.5.0) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl==1.5.0) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl==1.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pyro-ppl==1.5.0) (3.7.4.3)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOEvQiJMu9cQ"
      },
      "source": [
        "bnet = NN(512*4*4,1024,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VXBBfubuxQ8"
      },
      "source": [
        "def model(x_data, y_data):\n",
        "    \n",
        "    fc1w_prior = Normal(loc=torch.zeros_like(bnet.fc1.weight), scale=torch.ones_like(bnet.fc1.weight))\n",
        "    fc1b_prior = Normal(loc=torch.zeros_like(bnet.fc1.bias), scale=torch.ones_like(bnet.fc1.bias))\n",
        "    \n",
        "    outw_prior = Normal(loc=torch.zeros_like(bnet.out.weight), scale=torch.ones_like(bnet.out.weight))\n",
        "    outb_prior = Normal(loc=torch.zeros_like(bnet.out.bias), scale=torch.ones_like(bnet.out.bias))\n",
        "    \n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    # lift module parameters to random variables sampled from the priors\n",
        "    lifted_module = pyro.random_module(\"module\", bnet, priors)\n",
        "    # sample a regressor (which also samples w and b)\n",
        "    lifted_reg_model = lifted_module()\n",
        "    \n",
        "    lhat = log_softmax(lifted_reg_model(x_data))\n",
        "    \n",
        "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o_LPWnQu2AE"
      },
      "source": [
        "softplus = torch.nn.Softplus()\n",
        "\n",
        "def guide(x_data, y_data):\n",
        "    \n",
        "    # First layer weight distribution priors\n",
        "    fc1w_mu = torch.randn_like(bnet.fc1.weight)\n",
        "    fc1w_sigma = torch.randn_like(bnet.fc1.weight)\n",
        "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
        "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
        "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
        "    # First layer bias distribution priors\n",
        "    fc1b_mu = torch.randn_like(bnet.fc1.bias)\n",
        "    fc1b_sigma = torch.randn_like(bnet.fc1.bias)\n",
        "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
        "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
        "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
        "    # Output layer weight distribution priors\n",
        "    outw_mu = torch.randn_like(bnet.out.weight)\n",
        "    outw_sigma = torch.randn_like(bnet.out.weight)\n",
        "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
        "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
        "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
        "    # Output layer bias distribution priors\n",
        "    outb_mu = torch.randn_like(bnet.out.bias)\n",
        "    outb_sigma = torch.randn_like(bnet.out.bias)\n",
        "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
        "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
        "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    \n",
        "    lifted_module = pyro.random_module(\"module\", bnet, priors)\n",
        "    \n",
        "    return lifted_module()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRlk148u88q"
      },
      "source": [
        "optim = Adam({\"lr\": 0.01})\n",
        "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7oMUyFvvagB"
      },
      "source": [
        "Train Bayesian Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cr8V5MrvRWQ",
        "outputId": "9e561de9-a523-4414-fdcf-d84c6bfc516b"
      },
      "source": [
        "#load dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "#train network\n",
        "num_iterations = 3\n",
        "loss = 0\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    loss = 0\n",
        "    for batch_id, data in enumerate(trainloader):\n",
        "        # calculate the loss and take a gradient step\n",
        "        #print(data[0].size())\n",
        "        #print(data[1].size())\n",
        "        #print(data[1])\n",
        "        #data_new = data[0].view(-1,512*4*4)\n",
        "        #print(data_new.size())\n",
        "        loss += svi.step(data[0].view(-1,512*4*4), data[1])\n",
        "    normalizer_train = len(trainloader.dataset) \n",
        "    print(normalizer_train)\n",
        "    total_epoch_loss_train = loss / normalizer_train\n",
        "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "Epoch  0  Loss  17696.345529101865\n",
            "50000\n",
            "Epoch  1  Loss  771.588113425931\n",
            "50000\n",
            "Epoch  2  Loss  370.9168674097791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlSf19Gbo_Qf"
      },
      "source": [
        "Testing the State Dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuUeT0BnO43t",
        "outputId": "73257d2b-98db-414d-8b57-d3ea0ab5d497"
      },
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in bnet.state_dict():\n",
        "    print(param_tensor, \"\\t\", bnet.state_dict()[param_tensor].size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "fc1.weight \t torch.Size([1024, 8192])\n",
            "fc1.bias \t torch.Size([1024])\n",
            "out.weight \t torch.Size([10, 1024])\n",
            "out.bias \t torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ94kI1nPB9u"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVZhJ0Y3uKzi",
        "outputId": "386ba696-9746-4316-af28-3f6bd47ab3c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# good example: https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm.py\n",
        "\n",
        "# saves the model and optimizer states to disk\n",
        "def save_checkpoint():\n",
        "    PATH = '/content/gdrive/My Drive/Research/bnn/resnet/cifar10/bnet_state_dict.pth'\n",
        "    torch.save(bnet.state_dict(), PATH)\n",
        "    PATH = '/content/gdrive/My Drive/Research/bnn/resnet/cifar10/bnet_optim.pth'\n",
        "    optim.save(PATH)\n",
        "\n",
        "# loads the model and optimizer states from disk\n",
        "def load_checkpoint():\n",
        "    PATH = '/content/gdrive/My Drive/Research/bnn/resnet/cifar10/bnet_state_dict.pth'\n",
        "    bnet.load_state_dict(torch.load(PATH))\n",
        "    PATH = '/content/gdrive/My Drive/Research/bnn/resnet/cifar10/bnet_optim.pth'\n",
        "    optim.load(PATH)\n",
        "\n",
        "save_checkpoint()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7AODEB4O_Gb"
      },
      "source": [
        "Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwV60nq8ptmE"
      },
      "source": [
        "## Accuracy Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHo6E17EwTw6"
      },
      "source": [
        "BNN Test When Network Is *Forced* To Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5omC1LknuKIb"
      },
      "source": [
        "num_samples = 10\n",
        "def predict(x):\n",
        "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "    yhats = [model(x).data for model in sampled_models]\n",
        "    mean = torch.mean(torch.stack(yhats), 0)\n",
        "    return np.argmax(mean.numpy(), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbqjfZ61vi5f",
        "outputId": "de569f3c-e467-4bb0-8c89-f49a8b301918"
      },
      "source": [
        "\n",
        "#load dataset\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "print('Prediction when network is forced to predict')\n",
        "correct = 0\n",
        "total = 0\n",
        "for j, data in enumerate(testloader):\n",
        "    images, labels = data\n",
        "    predicted = predict(images.view(-1,512*4*4))\n",
        "    total += labels.size(0)\n",
        "    predicted = torch.Tensor(predicted)\n",
        "    correct += (predicted.eq(labels).sum().item())\n",
        "print('Accuracy of the BNN network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction when network is forced to predict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the BNN network on the 10000 test images: 94 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gy7APp9MUp7"
      },
      "source": [
        "#to optimize for space\n",
        "del trainset\n",
        "del trainloader\n",
        "del testset\n",
        "del testloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ice4Ek3Opex2"
      },
      "source": [
        "BNN Test When Network Can Be Undecided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbn7i88pD5kq"
      },
      "source": [
        "#BNN TEST WHEN IT CAN REFUSE\n",
        "\n",
        "num_samples = 100\n",
        "def give_uncertainities(x):\n",
        "      sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "      yhats = [F.log_softmax(model(x.view(-1,512*4*4)).data, 1).detach().numpy() for model in sampled_models]\n",
        "      return np.asarray(yhats)\n",
        "      #mean = torch.mean(torch.stack(yhats), 0)\n",
        "      #return np.argmax(mean, axis=1)\n",
        "\n",
        "def bnn_test(test_loader, plot=True):\n",
        "  predicted_for_images = 0\n",
        "  correct_predictions=0 \n",
        "  total = 0\n",
        "  for j, data in enumerate(test_loader):\n",
        "      images, labels = data\n",
        "      y = give_uncertainities(images)        \n",
        "\n",
        "      for i in range(len(labels)):\n",
        "          all_digits_prob = []\n",
        "          highted_something = False\n",
        "          for j in range(10):\n",
        "              highlight=False\n",
        "              histo = []\n",
        "              histo_exp = []\n",
        "              for z in range(y.shape[0]):\n",
        "                  histo.append(y[z][i][j])\n",
        "                  histo_exp.append(np.exp(y[z][i][j]))\n",
        "              prob = np.percentile(histo_exp, 10) #sampling median probability, sampling 10th percentile to be 50%\n",
        "              if(prob>0.5): #select if network thinks this sample is 50% chance of this being a label\n",
        "                  highlight = True #possibly an answer\n",
        "              all_digits_prob.append(prob)\n",
        "          \n",
        "              if(highlight):\n",
        "                  highted_something = True\n",
        "      \n",
        "          predicted = np.argmax(all_digits_prob)\n",
        "      \n",
        "          if(highted_something):\n",
        "              predicted_for_images+=1\n",
        "              if(labels[i].item()==predicted):\n",
        "                  correct_predictions +=1.0\n",
        "\n",
        "      total+= len(labels)\n",
        "      \n",
        "  if(plot):\n",
        "          print(\"Summary\")\n",
        "          print(\"Total images: \", total, \"10,000\")\n",
        "          print(\"Predicted for: \", predicted_for_images)\n",
        "          print(\"Correct Images \", correct_predictions)\n",
        "          print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
        "          good = ((total - predicted_for_images) + correct_predictions)*100 /total\n",
        "          print(\"Good Score\", good)\n",
        "\n",
        "  return total, predicted_for_images, int(correct_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "8QxpftOZM1IR",
        "outputId": "66cf42e8-6f92-4f2a-9bd9-d021be00ddd9"
      },
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.DataFrame(columns=['Sigmas','Total','Predicted','Correct'])\n",
        "\n",
        "sigmas = [0.05, 0.15, 0.25, 0.35, 0.45]\n",
        "sigma_list = []\n",
        "total = []\n",
        "predicted = []\n",
        "correct1 = []\n",
        "\n",
        "for sigma in sigmas:\n",
        "  correct_path = f\"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/testset_{sigma}_c.pth\"\n",
        "  correct = torch.load(correct_path, map_location='cpu')\n",
        "  testloader_c = torch.utils.data.DataLoader(correct, batch_size=128, shuffle=False, num_workers=2)\n",
        "  tot, pred, cor = bnn_test(testloader_c)\n",
        "  sigma_list.append(sigma)\n",
        "  total.append(tot)\n",
        "  predicted.append(pred)\n",
        "  correct1.append(cor)\n",
        "  print(\"Tested BNN on SNN correct data for sigma = \",sigma)\n",
        "  del correct\n",
        "  del testloader\n",
        "\n",
        "\n",
        "for sigma in sigmas:\n",
        "  incorrect_path = f\"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/testset_{sigma}_f.pth\"\n",
        "  incorrect = torch.load(incorrect_path, map_location='cpu')\n",
        "  testloader_f = torch.utils.data.DataLoader(incorrect, batch_size=128, shuffle=False, num_workers=2)\n",
        "  tot, pred, cor = bnn_test(testloader_f)\n",
        "  sigma_list.append(sigma)\n",
        "  total.append(tot)\n",
        "  predicted.append(pred)\n",
        "  correct1.append(cor)\n",
        "  print(\"Tested BNN on VGG incorrect data for sigma = \",sigma)\n",
        "\n",
        "dataframe['Sigmas'] = sigma_list\n",
        "dataframe['Total'] = total\n",
        "dataframe['Predicted'] = predicted\n",
        "dataframe['Correct'] = correct1\n",
        "\n",
        "dataframe_path = F\"/content/gdrive/My Drive/Research/bnn/restnet/cifar10/cifar10_noise_0.csv\"\n",
        "\n",
        "dataframe.to_csv(dataframe_path)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Summary\n",
            "Total images:  9490 10,000\n",
            "Predicted for:  0\n",
            "Correct Images  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ec8db2fb8f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtestloader_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnn_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0msigma_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mtotal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2e31694837e8>\u001b[0m in \u001b[0;36mbnn_test\u001b[0;34m(test_loader, plot)\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted for: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_for_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correct Images \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy when predicted: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpredicted_for_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m           \u001b[0mgood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted_for_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Good Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJIVG71yCFb4",
        "outputId": "fc503ed8-f89d-4f2c-ca6a-acd8b69040df"
      },
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.DataFrame(columns=['Epsilon','Total','Predicted','Correct'])\n",
        "\n",
        "epsilon_list  = [0,1,2,3,4,5,6,7,8,9]\n",
        "epsilon_strength = []\n",
        "total = []\n",
        "predicted = []\n",
        "correct1 = []\n",
        "\n",
        "\n",
        "for epsilon in epsilon_list:\n",
        "  correct_path = f\"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/attack_testsets/attack_testset_{epsilon}_c.pth\"\n",
        "  correct = torch.load(correct_path, map_location='cpu')\n",
        "  testloader_c = torch.utils.data.DataLoader(correct, batch_size=128, shuffle=False, num_workers=2)\n",
        "  tot, pred, cor = bnn_test(testloader_c)\n",
        "  epsilon_strength.append(epsilon)\n",
        "  total.append(tot)\n",
        "  predicted.append(pred)\n",
        "  correct1.append(cor)\n",
        "  print(\"Tested BNN on SNN correct data for epsilon = \",epsilon)\n",
        "  del correct\n",
        "  del testloader_c\n",
        "\n",
        "\n",
        "\n",
        "for epsilon in epsilon_list:\n",
        "  incorrect_path = f\"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/attack_testsets/attack_testset_{epsilon}_c.pth\"\n",
        "  incorrect = torch.load(incorrect_path, map_location='cpu')\n",
        "  testloader_f = torch.utils.data.DataLoader(incorrect, batch_size=128, shuffle=False, num_workers=2)\n",
        "  tot, pred, cor = bnn_test(testloader_f)\n",
        "  epsilon_strength.append(epsilon)\n",
        "  total.append(tot)\n",
        "  predicted.append(pred)\n",
        "  correct1.append(cor)\n",
        "  print(\"Tested BNN on VGG incorrect data for epsilon = \",epsilon)\n",
        "  del incorrect\n",
        "  del testloader_f\n",
        "\n",
        "dataframe['Epsilon'] = epsilon_strength\n",
        "dataframe['Total'] = total\n",
        "dataframe['Predicted'] = predicted\n",
        "dataframe['Correct'] = correct1\n",
        "\n",
        "dataframe_path = \"/content/gdrive/My Drive/Research/bnn/resnet/cifar10/adversarial_data.pth\"\n",
        "dataframe.to_csv(dataframe_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Summary\n",
            "Total images:  9528 10,000\n",
            "Predicted for:  8955\n",
            "Correct Images  8947.0\n",
            "Accuracy when predicted:  0.999106644332775\n",
            "Good Score 99.91603694374476\n",
            "Tested BNN on SNN correct data for epsilon =  0\n",
            "Summary\n",
            "Total images:  8313 10,000\n",
            "Predicted for:  6922\n",
            "Correct Images  6896.0\n",
            "Accuracy when predicted:  0.9962438601560243\n",
            "Good Score 99.68723685793336\n",
            "Tested BNN on SNN correct data for epsilon =  1\n",
            "Summary\n",
            "Total images:  8036 10,000\n",
            "Predicted for:  6633\n",
            "Correct Images  6606.0\n",
            "Accuracy when predicted:  0.9959294436906377\n",
            "Good Score 99.66401194624191\n",
            "Tested BNN on SNN correct data for epsilon =  2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}